Two group comparisons and data aggregation
======================================================================

Contributors: Jenny Bryan

```{r include = FALSE}
## once I upgrade knitr, "no tidy" will be the default and I can delete this
opts_chunk$set(tidy = FALSE)
```

We have goals on three different fronts today:

  * __Workstyle: reproducible research.__ Instead of typing in the Console, try keeping R code in a script, running it from there interactively, sourcing it as a whole, and/or making a nice HTML report from it. Accomplished easily via RStudio's "File --> Compile Notebook" command; requires the `knitr` add-on package.
  * __Statistical inference: two group comparisons.__ We will conduct various statistical tests for assessing whether two populations (or treatments or conditions or whatever) differ in distribution or in central location. Today, we're still doing this one gene at a time or for a small-ish number of genes (mass production of such tests comes later).
  * __Workflow: data aggregation.__ We often need to do a repetitive computational (or graphical task) for various subsets of an R object, especially data.frames and matrices. This is called "data aggregation" and R has fantastic facilities for this that will eliminate the need for most top-level, user-written `for()` loops. Learn to love the `apply` family of functions and/or the add-on package `plyr`.
  
## Reproducible research

*Note: this overlaps partially with previously recommended STAT 545A content: [R basics, workspace and working directory, RStudio projects](http://www.stat.ubc.ca/~jenny/STAT545A/block01_basicsWorkspaceWorkingDirProject.html). Skim or skip?*

When you are new to R, it is OK to perform your computations live in the Console, to allow your workspace to be saved / loaded repeatedly, and to mine your R history for "good" commands. But before long, I suggest you up your game and start to work in a way that guarantees you will be able to reproduce your analytical results and your figures. Keep your R commands in scripts, which shall have names ending in ".r" or ".R" by convention. Rest assured, you can execute the code easily in RStudio (or other IDEs) without literally copying from the script and pasting into the Console.

Two main approaches (you will likely end up using a hybrid):

  * Work interactively in the Console for a while. Once you've got some analysis worth preserving, go to the History tab in the upper right pane. Select the commands you need to keep and use the "To Source" button to create a new R script. In the upper left pane, you will now have an R script in an editor pane, probably called "Untitled1". Save with a descriptive name, e.g. "deaByGtype.R", for "differential expression analysis by genotype", remembering to use the ".r" or ".R" suffix. Assuming you're working in an RStudio project and you haven't done anything funny with your working directory, the script will be saved, logically, in the associated directory.
  * Create a new R script with "File --> New --> R script". Write your R commands in the script, in the editor pane, and send the commands to the Console with the mouse (explore the various "Run" options in the editor's upper right corner) or, *much better in the long run*, keyboard shortcuts. Once you've done a little work, save and name the script as described above.
  
Once your script exists, you can continue to do some work in the Console if you wish, select those commands in the History and use the "To Source" button to move them into the script. That is, the approaches above are both useful and are not mutually exclusive.

Once your script contains a coherent, complete analysis, you can create an easy low-tech report, containing your source code, and it's outputs. I urge you to always load data with commands, not the mouse, and to write files, including figures, with commands not the mouse.

> If you have not already installed `knitr`, you will need to do so now. Recall that can be done with `install.packages()`, among other ways.

With focus in the editor pane holding your script, use RStudio's "File --> Compile Notebook" command. Accept the default name and location, at least for today. A preview of the html "notebook" you've just created should pop up. You can also use RStudio's Files pane (or any other method) to verify that, e.g., "deaByGtype.html" now exists. Go visit the file in your browser and enjoy the warm, fuzzy feeling of having a complete automated analysis report. Read more about RStudio's notebooks [here](http://www.rstudio.com/ide/docs/authoring/markdown_notebooks).

*Note: we have already had some discussion of R Markdown in last week's seminar.*

For richer reports, you will eventually want to write in R markdown or LaTeX, instead of of just plain R code. These other formats allow for a mix of prose (including hyperlinks or mathematical notation), code, and the outputs of code, such as nicely formatted tables and figures. These seminar pages, for example, are authored in R markdown. Your early experimentation with "File-->Compile Notebook" puts you on a path to take advantage of a very powerful set of tools for reproducible research.  Read more about writing R Markdown within RStudio [here](http://www.rstudio.com/ide/docs/authoring/using_markdown). Read more about the `knitr` package which is powering this [here](http://yihui.name/knitr/).

I encourage you to start experimenting with keeping code in a script and generating R-based reports programmatically in today's seminar.

## Load the `photoRec` data and the `ggplot2` package

> Remember you may need to edit the file paths below, to reflect your working directory and local file storage choices.

```{r}
library(ggplot2)
prDat <- read.table("../examples/photoRec/data/GSE4051_data.tsv")
str(prDat, max.level = 0)
prDes <- readRDS("../examples/photoRec/data/GSE4051_design.rds")
str(prDes)
```

## Two sample tests -- one gene

Let's extract the data for one gene and put in a data.frame with the experimental information.
```{r}
set.seed(987)
(theGene <- sample(1:nrow(prDat), 1))
pDat <- data.frame(prDes, gExp = unlist(prDat[theGene, ]))
str(pDat)
```

Always explore the data before plunging into analysis! What are the sample means in the wild type and Nrl knockout groups (yes, we're ignoring developmental stage today)? (`aggregate()` and other data aggregation functions are explained below.)

```{r}
aggregate(gExp ~ gType, pDat, FUN = mean)
```

Let's make a stripplot so we can sanity test our $t$ test result.

```{r}
ggplot(pDat, aes(x = gExp, y = gType)) + geom_point()
```

We will do a two-sample $t$ test comparing wild type to the Nrl knockouts.

```{r}
t.test(gExp ~ gType, pDat)
```

If we save the $t$ test result, we can inspect what it is.

```{r}
ttRes <- t.test(gExp ~ gType, pDat)
str(ttRes)
```
It is a `list`; some of the more interesting and useful components are the test statistic and p-value. Recall how to extract components of a list:
```{r}
ttRes$statistic
ttRes$p.value
```

You try: draw a different gene at random or pick one for biological interest and look up the Affy probe ID. Use the $t$ test, with and without the common variance assumption, the Wilcoxon, and/or the Kolmogorov-Smirnov test to assess differential expression. Can you pull test statistics and/or p-values from the different approaches into an common object, like a readable table? Are you getting the same message from the various approaches? Hint: `wilcox.test()`, `ks.test()`.

## What is data aggregation?

Warning: it is impossible to cover data aggregation without confronting the different flavors of R objects, e.g. vectors vs. matrices vs. data.frames vs. lists. Expect to encounter technical details about the R language. You do not need to master all of this today (or, perhaps, ever), so get what you can out of it and revisit later if needed.

Please read some content from STAT 545A (all drawn from [Data aggregation](http://www.stat.ubc.ca/~jenny/STAT545A/block04_dataAggregation.html)). Specific sub-sections for now:

  * [Data aggregation overview](http://www.stat.ubc.ca/~jenny/STAT545A/block04_dataAggregation.html#data-aggregation-1)
  * [Data aggregation landscape](http://www.stat.ubc.ca/~jenny/STAT545A/block04_dataAggregation.html#data-aggregation-landscape)
  
## `apply()` for computing on rows and columns of matrices

Even though data.frames are at the heart of most analyses, let's start at the beginning with `apply()`, which operates on a matrix (or arrays more generally). `apply()` is a built-in base R function; it is not part of `plyr`. Recall that arrays can only hold info all of the same "flavor", such as numeric.

We can get a numeric matrix easily from the gene expression variables in the small excerpt of `photoRec` that we've worked with before.

```{r}
kDat <- readRDS("../examples/photoRec/data/GSE4051_MINI.rds")
kMat <- as.matrix(kDat[c('crabHammer', 'eggBomb', 'poisonFang')])
str(kMat)
```

Let's compute the median expression for specific genes (= *column*), "by hand" and using `apply()`.
```{r}
median(kMat[ , 1])         # column numbers are mysterious
median(kMat[ , 'eggBomb']) # use names for better code!
apply(kMat, 2, median)
## apply usage: apply(X, MARGIN, FUN, ...)
```

The first argument of `apply()` is the matrix, the second is the dimension(s) to operate on (`1` means rows, `2` means columns, and so on), the third is the function to apply, which can be built-in like `median()` above, custom defined by you elsewhere, or custom defined by you "on the fly". [Reading the help file](http://stat.ethz.ch/R-manual/R-devel/library/base/html/apply.html), you will also notice the weird `...` argument where you can specify arbitrary arguments that will be passed through to the function specified via `FUN =`. Here's a alternative way to compute gene-specific medians (or other quantiles) using this argument.

```{r}
apply(kMat, 2, quantile, probs = 0.5)
apply(kMat, 2, quantile, probs = c(0.25, 0.75))
```

Let's take the minimum gene expression for each sample, across these three genes. Then let's determine *which gene* contributed that maximum value.

```{r}
apply(kMat, 1, min)
colnames(kMat)[apply(kMat, 1, which.min)]
```

Computing row- and column-wise sums and means is such an important special case that there are purpose-built and fast functions for this that I recommend you use when relevant.

```{r}
rowSums(kMat) #see also rowMeans, colMeans, colSums
all.equal(rowSums(kMat), apply(kMat, 1, sum))
colMeans(kMat)
all.equal(colMeans(kMat), apply(kMat, 2, mean))
```

Obviously you won't notice the performance advantage in `kMat` but we can detect it with the entire dataset `prDat`, where we get a substantial speed-up (albeit of little practical significance.) For a dorky little timing study, see the appendix. __More important than the superiority of `rowMeans()` over `apply(..., 1, mean)` is the superiority of *either* over a top-level `for()` loop__. Compare these two elegant little statements, short and self-documenting:
```{r}
jRowSums <- rowSums(prDat)
jRowSums <- apply(prDat, 1, sum)
```
to this hot mess:
```{r}
prMat <- as.matrix(prDat) # must have an actual matrix
jRowSums <- rep(NA, nrow(prDat)) # must initialize a receptacle
for(i in 1:nrow(prDat)) {
   jRowSums[i] <- sum(prMat[i, ])
}
```
Yeah, yeah, performance advantage is there but the __real reason__ to use `apply()` functions is that your code is easier to write and easier to read. You also don't have to set up a "storage receptacle" ahead of time, which becomes really nice when you are less certain exactly what sort of output is coming back at you (more later).

Due to historical problems with lousy memory management, there is a special horror around `for()` loops in R, although they can be useful. To be clear, of course computations like row-wise means require a `for` loop to happen somewhere. When you use the `apply()` family functions this loop is often happening down at a very low level (i.e. down in C, which is what R is written in) and implemented in code written by a better programmer than you or I. And therefore you use less RAM and time, sometimes dramatically less. __But the real reason to embrace data aggregation facilities is the huge improvement in code readability and writability.__

## Computing on groups of observations with `aggregate()`

More typical -- and conceptually trickier -- than the row- and column-wise operations above are operations on groups of observations, where the groups are induced by the levels of some factor (or combinations of multiple factors). We re-focus on data.frames, which is our go-to data receptacle.

Let's compute on a quantitative variable, based on the levels of a factor using the built-in function `aggregate()`. Specifically, let's compute average expression of `eggBomb` for different levels of `devStage`.

```{r}
aggregate(eggBomb ~ devStage, kDat, FUN = mean)
```

The call has familiar elements: a formula `y ~ x` reminiscent of other modelling and graphing calls, a data.frame where the variables are, and a function to apply. [Read the documentation](http://stat.ethz.ch/R-manual/R-devel/library/stats/html/aggregate.html) to learn more.

We can split the data into groups based on a *combination* of factors.

```{r}
aggregate(eggBomb ~ gType * devStage, kDat, FUN = mean)
```

We are not limited to computing a single value for each group. Although it's silly with such a small dataset, we can use `range()` to report the min and max.

```{r}
aggregate(eggBomb ~ gType * devStage, kDat, FUN = range)
```

## Two sample tests -- a handful of genes

Let's grab the data from 6 genes. I've picked them for you: 3 are interesting ('hits'), 3 are not. I also reshape the data to be tall and skinny, which is generally a good policy and allows us to keep learning more about data aggregation.

```{r, eval=FALSE, hide=TRUE, echo=FALSE}
load("../data/photoRec/deGtype.robj")
str(deGtype)
set.seed(345)
(getMe <- c(sample(1:50, size = 3), sample(25000:29000, size = 3)))
getMe <- sort(getMe)
match(getMe, deGtype$rank)
rownames(prDat)[match(getMe, deGtype$rank)]
```

```{r}
keepGenes <- c("1431708_a_at", "1424336_at", "1454696_at",
               "1416119_at", "1432141_x_at", "1429226_at" )
miniDat <- subset(prDat, rownames(prDat) %in% keepGenes)
miniDat <- data.frame(gExp = as.vector(t(as.matrix(miniDat))),
                      gene = factor(rep(rownames(miniDat), each = ncol(miniDat)),
                                    levels = keepGenes))
miniDat <- suppressWarnings(data.frame(prDes, miniDat))
str(miniDat)
```

Let's plot to make sure we have successfully gotten 3 clear 'hits' and 3 clear boring genes, as promised.

```{r}
ggplot(miniDat, aes(x = gExp, y = gType, color = gType)) +
  facet_wrap(~ gene, scales="free_x") +
  geom_point(alpha = 0.7) +
  theme(panel.grid.major.x = element_blank())
```
Smells "right": bottom row consists of 3 'hits', top row holds the boring genes.

Let's use data aggregation techniques to conduct some two group comparisons for each of these 6 genes. Recall the syntax of the two-sample t-test for one gene:

```{r eval=FALSE}
t.test(gExp ~ gType, someDat)
```

Conceptually, we want to make a sub-data.frame for each gene and provide in the place of `someDat` in a $t$ test call like above. Sometimes that is a useful first step, when building up a data aggregation task. Walk before you run.
```{r}
someDat <- droplevels(subset(miniDat, gene == keepGenes[1]))
t.test(gExp ~ gType, someDat)
```

How do we scale this up to all 6 genes? We have now outgrown the capability of `aggregate()`. If we restrict ourselves to the built-in functions, we'd need to look at functions like `tapply()`, `split()`, and `by()`. However I think it's the right time to start using `plyr`.

## The `plyr` package

Please read some content from STAT 545A (all drawn from [Data aggregation](http://www.stat.ubc.ca/~jenny/STAT545A/block04_dataAggregation.html)). Specific sub-sections:

  * [Install and load `plyr`](http://www.stat.ubc.ca/~jenny/STAT545A/block04_dataAggregation.html#install-and-load-plyr)
  * [`plyr` Big Ideas](http://www.stat.ubc.ca/~jenny/STAT545A/block04_dataAggregation.html#plyr-big-ideas)
  
Do this if you don't have `plyr` yet:
```{r eval=FALSE}
install.packages(pkgs = "plyr")
```

Since our input, `miniDat`, is a data.frame, we will use functions that start with `d`. What do we want to get back, if anything? If we are happy to watch the $t$ test results fly by on the screen, we can use `d_ply()`:

```{r}
library(plyr)
d_ply(miniDat, ~ gene, function(x) t.test(gExp ~ gType, x), .print = TRUE)
```

That's not so helpful: the results aren't labelled by probeset and whiz by. In real life, you will want these results for further processing, e.g. writing to file or presenting in a table. We know that `t.test()` returns a list, so we can use `dlply()` to retain everything in a new list with one component per probeset:

```{r}
ttRes <- dlply(miniDat, ~ gene, function(x) t.test(gExp ~ gType, x))
names(ttRes)
ttRes[["1454696_at"]]
```

We could then process this list further with `plyr` functions that start with `l`. If we knew in advance that we only wanted, say, the test statistic and the p-value, here's how we go after that directly via `ddply()`:

```{r}
ttRes <- ddply(miniDat, ~ gene, function(z) {
  zz <- t.test(gExp ~ gType, z)
  round(c(tStat = zz$statistic, pVal = zz$p.value), 4)
})
ttRes
```

We've now conducted two group comparisons for all 6 genes at once without ever writing a top-level `for` loop. Use data aggregation functions! Long-term, I strongly recommend use of `plyr` over the built-in `apply()` functions, due to its logical and very general framework.

## Ideas for take-home work

In our last example, can you edit the inner function to use the Wilcoxon or KS test? Or maybe do the $t$ test, the Wilcoxon, and the KS test and return all 3 p-values?

Scale up to more genes ... I'm thinking ~100. 

Make a numeric matrix with p-values for many genes (= rows) for various two-group tests. Scatterplot them against each other. Are the p-values as similar as you'd expect? Do you need to log transform the axes to get more insight?

Convert your numeric matrix of p-values into a matrix of `TRUE/FALSE` or zeros and ones by hard-threshholding, e.g. at the conventional 0.05 level. Use `apply()` or a function from `plyr` to make some interesting row or column summaries. How many genes are significant according to the different tests? For each gene, how many of the tests return a significant p-value? How many genes are "hits" by all 3 methods, by exactly 2, by exactly 1 and by none?

Use `system.time()` to do a little timing study, doing differential expression analysis for gene sets of various sizes. I'm thinking 5, 10, 50, 100, 500 ... somthing like that. Don't immediately go all the way to 30K. We will use different strategies for our full scale-up. Plot the DE analysis time against the number of genes.

Keep working towards:

  * retaining complete, coherent analyses in individual ".r" or ".R" scripts
  * turning those into analysis reports with RStudio's "Compile Notebook" command
  * pushing the code to github as a Gist or a file in a repo
  * pushing the pretty report to the web, either to RPubs or in Markdown or HTML format in a github repo

## Appendix: timing of different ways to, e.g., take row or column means

```{r}
(rowSumsTime <- system.time(foo <- rowSums(prDat)))
(applyRowSumTime <- system.time(foo <- apply(prDat, 1, sum)))
applyRowSumTime / rowSumsTime
```
```{r}
(colSumsTime <- system.time(foo <- colSums(prDat)))
(applyColSumTime <- system.time(foo <- apply(prDat, 2, sum)))
applyColSumTime / colSumsTime
```

```{r}
forLoopRowSumTime <- system.time({
  prMat <- as.matrix(prDat)
  mySums <- rep(NA, nrow(prDat))
  for(i in 1:nrow(prDat)) {
   mySums[i] <- sum(prMat[i, ])
  }
})
forLoopRowSumTime/rowSumsTime
```
