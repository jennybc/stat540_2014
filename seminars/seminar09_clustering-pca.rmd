Seminar 9: Cluster Analysis and PCA
==========================================

Contributor: Gabriela Cohen Freue

## Introduction

In this seminar we'll explore clustering genes and samples using the photoreceptor time series with the two genotypes. It is interesting to compare the results of different clustering algorithms, and to see the effect of filtering and/or definitions of attributes on the resulting clusters.  

## Data Preparation

Load the `photoRec` data. Remember, for more information on the `photoRec` dataset, go [here](https://github.com/jennybc/stat540_2014/tree/master/examples/photoRec). 

Install required packages if you haven't done so before. This seminar will require `pvclust`, `xtable`, `limma`, `lattice`, `cluster`, and `RColorBrewer`.

> Remember you may need to edit the file paths below, to reflect your working directory and local file storage choices.

```{r}
library(RColorBrewer)
library(cluster)
library(pvclust)
library(xtable)
library(limma)
library(lattice)
```

```{r}
prDat <- read.table("../examples/photoRec/data/GSE4051_data.tsv",
                    header = TRUE, row.names = 1) # the whole enchilada
str(prDat, max.level = 0)
prDes <- readRDS("../examples/photoRec/data/GSE4051_design.rds")
str(prDes)
```

Finally, as an additional step to make visualization easier later, we'll rescale the rows, since we're not interested in absolute differences in expression between genes at the moment. Note that although one can do this step within the `heatmap()` function, it will not be available for other functions we will use. We can always go back to the original data if we need to.

```{r}
sprDat <- t(scale(t(prDat)))
str(sprDat, max.level = 0)
data.frame(orig = rowMeans(head(prDat)), postScale = rowMeans(head(sprDat)))
data.frame(orig = apply(head(prDat), 1, var),
           postScale = apply(head(sprDat), 1, var))
```

The data for each row -- which is for one probeset -- now has mean 0 and variance 1.
        
## Sample Clustering

In this part, we will use samples as objects to be clustered using gene attributes (i.e., vector variables of dimension ~30K). 

### Hierarchical clustering for `photoRec` data

In this section we will illustrate different hierarchical clustering methods. These plots were included in Lecture 16.

However, for most expression data applications, we suggest you should standardize the data; use Euclidean as the "distance" (so it's just like Pearson correlation) and use "average linkage". 

```{r}
# compute pairwise distances
pr.dis <- dist(t(sprDat), method = 'euclidean')

# create a new factor representing the interaction of gType and devStage
prDes$grp <- with(prDes, interaction(gType, devStage))
summary(prDes$grp)

# compute hierarchical clustering using different linkage types
pr.hc.s <- hclust(pr.dis, method = 'single')
pr.hc.c <- hclust(pr.dis, method = 'complete')
pr.hc.a <- hclust(pr.dis, method = 'average')
pr.hc.w <- hclust(pr.dis, method = 'ward')

# plot them
par(mar = c(0,4,4,2))
par(mfrow = c(2,2))

plot(pr.hc.s, labels = FALSE, main = "Single", xlab = "")

plot(pr.hc.c, labels = FALSE, main = "Complete", xlab = "")

plot(pr.hc.a, labels = FALSE, main = "Average", xlab = "")

plot(pr.hc.w, labels = FALSE, main = "Ward", xlab = "")

# identify 10 clusters

par(mfrow = c(1,1))

plot(pr.hc.w, labels = prDes$grp, cex = 0.6, 
     main = "Ward showing 10 clusters")
rect.hclust(pr.hc.w, k = 10)
```

When you call `heatmap()`, it automatically performs hierarchical clustering for you and it reorders the rows and/or columns of the data accordingly. Both the reordering and the dendrograms can be suppressed it with `Rowv = NA` and/or `Colv = NA`. 

> Note that when you have a lot of genes, the tree is pretty ugly. Thus, the row clustering was suppressed for now.

By default, `heatmap()` uses the `hclust()` function, which takes a distance matrix, calculated by the `dist()` function (with `default = 'euclidean'`). However, you can also write your own clustering and distance functions. In the examples below, I used `hclust()` with `ward` linkage method and the `euclidean` distance. 

> Note that the dendrogram in the top margin of the heatmap is the same as that of the `hclust()` function.

__Exercise__: Play with the options of the heatmap function and compare the different heatmaps. Note that one can also use the original data `prDat` and set the option `scale = "row"`. You will get the same heatmaps although the columns may be ordered differently (use `Colv = NA` to suppress reordering).


```{r}
jGraysFun <- colorRampPalette(brewer.pal(n = 9, "Greys"))
gTypeCols <- brewer.pal(11, "RdGy")[c(4,7)]
heatmap(as.matrix(sprDat), Rowv = NA, Colv = NULL, col = jGraysFun(256),
        hclustfun = function(x) hclust(x, method = 'ward'),
        distfun = function(x) dist(x, method = 'euclidean'),
        scale = "none", labCol = prDes$grp, labRow = NA, margin = c(8,1),
        ColSideColor = gTypeCols[unclass(prDes$gType)])
legend("topright", levels(prDes$gType),
       col = gTypeCols, lty = 1, lwd = 5, cex = 0.5)
```

```{r,echo=FALSE,eval=FALSE}
devStageCols <- brewer.pal(11, "RdGy")[c(2,4,7,9,11)]
heatmap(as.matrix(sprDat), Rowv = NA, Colv = NULL, col = jGraysFun(256),
        hclustfun = function(x) hclust(x, method = 'ward'),
        distfun = function(x) dist(x, method = 'euclidean'),
        scale = "none", labCol = prDes$grp, labRow = NA, margin = c(8,1),
        ColSideColor = devStageCols[unclass(prDes$devStage)])
```

### Partitioning methods for `photoRec` data

> Note that the results depend on the initial values (randomly generated) to create the first k clusters. In order to get the same results, you need to set many initial points (see the parameter `nstart`).

#### K-means clustering

K-means is a classic clustering method described in Lecture 16. An important observation about k-means is that it cannot determine the number of clusters for you. In fact, doing this automatically is quite hard (though techniques do exist).

Here we'll just do a clustering of samples using all genes (~30K).

```{r}
#Objects in columns

set.seed(31)

samplecenters <- 5
pr.km <- kmeans(t(sprDat), centers = samplecenters, nstart =  50)

#We can look at the within sum of squares of each cluster
pr.km$withinss

#We can look at the composition of each cluster

pr.kmTable <- data.frame(devStage = prDes$devStage, cluster = pr.km$cluster)
prTable  <-  xtable(with(pr.kmTable, table(devStage,cluster)),
                    caption='Number of samples from each develomental stage within each cluster')
```

```{r, results = 'asis'}
align(prTable) <- "lccccc"
print(prTable, type = 'html', caption.placement = 'top')
```

> Repeat the analysis using a different seed and check if you get the same clusters.

Helpful info and tips:

  * An aside on `set.seed()`: Normally you might not need to set this; R will pick one. But if you are doing a "real" experiment and using methods that require random number generation, you should consider it when finalizing an analysis. The reason is that your results might come out slightly different each time you run it. To ensure that you can exactly reproduce the results later, you should set the seed (and record what you set it to). Of course if your results are highly sensitive to the choice of seed, that indicates a problem. In the case above, we're just choosing genes for an exercise so it doesn't matter, but setting the seed makes sure all students are looking at the same genes. 


#### PAM algorithm

K representative objects (= medoids) are chosen as cluster centers and objects are assigned to the center (= medoid = cluster) with which they have minimum dissimilarity (Kaufman and Rousseeuw, 1990). Nice features of PAM are: (a) it accepts a dissimilarity matrix (use `diss = TRUE`); (b) it is more robust to outliers as the centroids of the clusters are data objects; (c) one can determine the number of clusters by exploring the average silhouette value.

```{r}
pr.pam <- pam(pr.dis, k = samplecenters)
split(as.vector(unlist(prDes$devStage)), pr.pam$clustering)
```

> Additional information on the PAM result is available through `summary(pr.pam)`

**The silhouette plot**
The `cluster` package contains the function `silhouette()` that compares the minimum average dissimilarity of each object to other clusters __with__ the average dissimilarity to objects in its own cluster. The resulting measure is called the "width of each object's silhouette". A value close to 1 indicates that the object is similar to objects in its cluster compared to those in other clusters. Thus, the average of all objects silhouette widths gives an indication of how well the clusters are defined. 

```{r}
op <- par(mar = c(5,1,4,4))
plot(pr.pam, main = "Silhouette Plot for 5 clusters")
par(op)
```

> Exercise: draw a plot with number of clusters in the x-axis and the average silhouette widths in the y-axis. Use the information obtained to determine if 5 was the best choice for the number of clusters.

## Gene clustering

A different view at the data can be obtained from clustering genes instead of samples. Since clustering genes is slow when you have a lot of genes, for the sake of time we will work with a smaller subset of genes. 

In many cases, analysts use cluster analysis to illustrate the results of a differential expression analysis. Sample clustering following a differential expression (DE) analysis will probably show the separation of the groups identified by the DE analysis. Thus, as it was mentioned in lectures, we need to be careful in over-interpreting these kind of results. However, note that it is valid to perform a gene clustering to see if differential expressed genes cluster according to their function, subcellular localizations, pathways, etc.

####  A smaller dataset 

In [Seminar 6: Fitting and interpretting linear models (high volume)](seminar06_highVolumeLinearModelling.html), you've learned how to use `limma` to fit a common linear model to a very large number of genes and thus identify genes that show differential expression over the course of development.  


```{r echo=FALSE}
cutoff <-  1e-05
DesMat <- model.matrix(~ devStage, prDes)
dsFit <- lmFit(prDat, DesMat)
dsEbFit <- eBayes(dsFit)
dsHits <- topTable(dsEbFit,
                   coef = grep("devStage", colnames(coef(dsEbFit))),
                   p.value = cutoff, n = Inf)
numBHhits <- nrow(dsHits)

topGenes <- rownames(dsHits)

#Scaled data of topGenes
topDat <- sprDat[topGenes, ]
```

We start by using different clustering algorithms to cluster the top `r numBHhits` genes that showed differential expression across the different developmental stage (BH adjusted p value < `r cutoff`).

#### Hierarchical:

```{r}

geneC.dis <- dist(topDat, method = 'euclidean')

geneC.hc.a <- hclust(geneC.dis, method = 'average')

plot(geneC.hc.a, labels = FALSE,
     main = "Hierarchical with Average Linkage", xlab = "")
```

When there are lots of objects to cluster, the dendrograms are in general not very informative as it is difficult to identify any interesting pattern in the data.

#### Partitioning

The most interesting thing to look at is the cluster centers (basically the "prototype" for the cluster) and membership sizes. Then we can try to visualize the genes that are in each cluster.

Let's visualize a cluster (remember the data were rescaled) using line plots. This makes sense since we also want to be able to see the cluster center.

```{r}
set.seed(1234)
genecenters <- 5
kmeans.genes <- kmeans(topDat, centers = genecenters)

# choose which cluster we want
clusterNum <- 1 

# Set up the axes without plotting; ylim set based on trial run.
plot(kmeans.genes$centers[clusterNum,], ylim = c(-4,4), type = 'n',
     xlab = "Samples", ylab = "Relative expression" ) 

# Plot the expression of all the genes in the selected cluster in grey. 
matlines(y = t(topDat[kmeans.genes$cluster == clusterNum,]), col='grey') 

# Add the cluster center. This is last so it isn't underneath the members
points(kmeans.genes$centers[clusterNum, ], type = 'l') 

# Optional: colored points to show which development stage the samples are from.
points(kmeans.genes$centers[clusterNum, ],  col = prDes$devStage, pch = 20) 
```

> Improve the plot above adding sample names to the x-axis (e.g., wt_E16_1)

#### Or, probably more commonly used, we can see both dendrograms using heatmaps (through hierarchical clustering): :

```{r}
heatmap(as.matrix(topDat), Rowv = NULL, Colv = NULL, col = jGraysFun(256),
        hclustfun = function(x) hclust(x, method = 'average'),
        distfun = function(x) dist(x, method = 'euclidean'),
        labCol = prDes$grp, labRow = NA, margin = c(8,1), scale = "none",
        ColSideColor = devStageCols[unclass(prDes$devStage)])
legend("topleft", levels(prDes$devStage), col = devStageCols,
       lty = 1, lwd = 5, cex = 0.5)
```

### Redefining the attributes

In the previous example, all the samples were used as attributes to cluster genes. However, we can define different attributes, for example, by estimating parameters of a linear model. Consider:

$$
  \begin{equation}
  X_{gi,devStage} = \mu_{g,devStage} + \epsilon_{gi,devStage}
  \end{equation}
$$

Thus, we can define a new attributes for each gene, i.e., $$Att_g=(\mu_{g,E16},\mu_{g,P2},\mu_{g,P6},\mu_{g,P10},\mu_{g,4w})$$ and estimate these parameters.

```{r,echo=FALSE,results='hide',eval=FALSE,include = FALSE}
# Not very interesting results. I will omit this example from lecture and
# seminar. As develomental stage is a time variable, another interesting model
# for these data is:
library(car) # for recode()

prDes$age <-
    recode(prDes$devStage,
           "'E16'=-2; 'P2'=2; 'P6'=6; 'P10'=10; '4_weeks'=28",
           as.factor.result = FALSE)

source("../examples/photoRec/code/80_anova-mlm.R")

prMat <- t(as.matrix(prDat))

#Fit a linear model with linear and quadratic time covariates
rQuadFit <- lm(prMat ~ age + I(age^2), prDes)
rConst <- lm(prMat ~ 1)

rqSumm <- summary(rQuadFit)
rfAnova <- anova(rQuadFit, rConst)

#Store coefficients:
deTime.coef <-
  data.frame(rqSumm$Coef[ ,c("age","I(age^2)"), "Estimate"],
             pval = rqSumm$Coef[ , c("age","I(age^2)"), "Pr(>|t|)"])

geneTimeDat <-
  deTime.coef[ , setdiff(1:ncol(deTime.coef),
                         grep("pval", colnames(deTime.coef)))]  

geneTime.km <- kmeans(geneTimeDat, centers = 5, nstart = 50)
# Warning messages:
# 1: Quick-TRANSfer stage steps exceeded maximum (= 1497450) 
# 2: Quick-TRANSfer stage steps exceeded maximum (= 1497450) 

plot(geneTimeDat, pch = geneTime.km$cluster, xlim = c(-.2,.2))

#Evidence of a temporal trend 
oTime.t <- deTime.coef[order(deTime.coef[ ,"pval.age"]), ]
topTime.t <- oTime.t[1:500, setdiff(1:ncol(deTime.coef),
                                    grep("pval", colnames(deTime.coef)))]

geneTime.km <- kmeans(topTime.t, centers = 2, nstart = 50)
plot(topTime.t, col = geneTime.km$cluster, xlab = "trend", ylab = "shape")

#Evidence of a temporal trend and curvature
oTime.t2 <-
  deTime.coef[deTime.coef[,"pval.age"] < 0.05 &
                deTime.coef[ ,"pval.I.age.2."] < 0.05, ]
topTime.t2 <-
  oTime.t2[1:500, setdiff(1:ncol(deTime.coef),
                          grep("pval", colnames(deTime.coef)))]

geneTime.km <- kmeans(topTime.t2, centers = 4, nstart = 50)
plot(topTime.t2, col = geneTime.km$cluster, xlab = "trend", ylab = "shape")
```

```{r}
oDat <-
  with(prDes,
       data.frame(sidNum, devStage, gType,
                  probeset = factor(rep(rownames(topDat), each = ncol(topDat))),
                  geneExp = as.vector(unlist(t(topDat)))))

devStageAvg <- with(oDat, tapply(geneExp, list(probeset, devStage), mean))
```


```{r,eval=FALSE}
heatmap(as.matrix(devStageAvg), Rowv = NULL, Colv = NA, col = jGraysFun(256),
        hclustfun = function(x) hclust(x,method = 'average'),
        distfun = function(x) dist(x,method = 'euclidean'),
        labCol = colnames(devStageAvg), labRow = NA, margin = c(8,1))
```

We can look at the average expression of genes within a cluster for each developmental stage.

```{r}
geneDS.km <- kmeans(devStageAvg, centers = 4, nstart = 50)

# Reorder columns of devStageAvg first
devStageAvg <- devStageAvg[ , levels(prDes$devStage)]
clust.centers <- geneDS.km$centers
clust.centers <-clust.centers[ , levels(prDes$devStage)]

#Look at all clusters
par(mfrow=c(2,2))
for(clusterNum in 1:4) {
  # Set up the axes without plotting; ylim set based on trial run.
  plot(clust.centers[clusterNum,], ylim = c(-4,4), type='n',
       xlab = "Develomental Stage", ylab = "Relative expression",
       axes = F, main = paste("Cluster", clusterNum, sep = " ")) 
  axis(2)
  axis(1, 1:5, c(colnames(clust.centers)[1:4],"4W"), cex.axis = 0.9)
  
  # Plot the expression of all the genes in the selected cluster in grey.
  matlines(y = t(devStageAvg[geneDS.km$cluster == clusterNum, ]),
           col = 'grey') 
  
  # Add the cluster center. This is last so it isn't underneath the members
  points(clust.centers[clusterNum, ] , type = 'l') 
  
  # Optional: points to show development stages.
  points(clust.centers[clusterNum, ],  pch = 20)
  } 
```

Or we can compare all clusters' centers.
```{r}
par(mfrow = c(1,1))

plot(clust.centers[clusterNum,], ylim = c(-4, 4), type = 'n',
     xlab = "Develomental Stage", ylab = "Average expression",
     axes = FALSE, main = "Clusters centers") 
axis(2)
axis(1, 1:5, c(colnames(clust.centers)[1:4],"4W"), cex.axis = 0.9)

for(clusterNum in 1:4) {
  points(clust.centers[clusterNum,], type = 'l', col = clusterNum, lwd=2) 
  points(clust.centers[clusterNum,] , col = clusterNum, pch = 20)
  }
```

We can look at 3-dimensions of the data and illustrate clusters determined by kmeans. The most interesting analysis is to follow with a biological interpretation of the clusters. For that, smaller clusters may be easier to interpret.

```{r}
cloud(devStageAvg[ ,"E16"] ~ devStageAvg[ ,"P6"] *
        devStageAvg[ ,"4_weeks"], col = geneDS.km$clust,
      xlab = "E16", ylab = "P6", zlab = "4_weeks")
```

## Statistical measures to evaluate clusters

An important issue for clustering is the question of certainty of the cluster membership. Clustering always gives you an answer, even if there aren't really any underlying clusters. There are many ways to address this. Here we introduce an approachable one offered in R, `pvclust`, which you can read about at (<http://www.is.titech.ac.jp/~shimo/prog/pvclust/>).

> Important: `pvclust` clusters the columns. I don't recommend doing this for genes! The computation will take a very long time. Even the following example with all 30K genes will take some time to run.

> You control how many bootstrap iterations `pvclust` does with the `nboot` parameter. We've also noted that `pvclust` causes problems on some machines, so if you have trouble with it, it's not critical. 

```{r}
pvc <- pvclust(topDat, nboot = 100)
plot(pvc, labels = prDes$grp, cex = 0.6)
pvrect(pvc, alpha = 0.95) 
```

## PCA (principal components analysis)

In R, we can use `prcomp()` to do PCA. You can also use `svd()`. The following code reproduces some of the material shown in the PCA/SVD lecture. (The genes used are not the same so it won't be exactly equivalent)

> Scaling is suppressed because we already scaled the rows. You can experiment with this to see what happens.

```{r}
pcs <- prcomp(sprDat, center = F, scale = F) 

# scree plot
plot(pcs) 

# append the rotations for the first 10 PCs to the phenodata
prDes<- cbind(prDes, pcs$rotation[prDes$sidNum, 1:10]) 

# scatter plot showing us how the first few PCs relate to covariates
plot(prDes[ ,c("sidNum", "devStage", "gType", "PC1", "PC2", "PC3")],
     pch=19, cex=0.8) 

# plot data on first two PCs, colored by development stage
plot(prDes[ ,c("PC1","PC2")], bg = prDes$devStage, pch=21, cex=1.5)
legend(list(x = 0.2, y = 0.3), as.character(levels(prDes$devStage)),
       pch = 21, pt.bg = c(1,2,3,4,5))
```

It is commonly seen a cluster analysis on the first 3 principal components to illustrate and explore the data. 

> Most of the plots in this Seminar were done with basic R graphics. As an exercise, you can try to create new plots using `lattice` and/or `ggplot2`!
